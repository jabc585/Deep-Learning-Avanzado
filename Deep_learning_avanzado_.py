# -*- coding: utf-8 -*-
"""DEEP LEARNING AVANZADO .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gvHPmhGxzxCrtK49h9NR5Ltwa6KhvMuH

<h1><font color="#113D68" size=5>Redes neuronales y deep learning</font></h1>

# Datos



El objetivo de este Proyecto es simular como se haría un análisis completo de un problema para resolverlo con Deep Learning. Nos pondremos en la piel de un data scientist dedicado a analizar y crear modelos de Deep Learning para pasarlos a producción y ser desplegados en una aplicación.

Destacar que este Proyecto es la continuación de la última actividad realizada en la semana anterior. En la actividad de la semana anterior encontramos la mejor arquitectura para los datos que tenemos y ahora vamos a realizar más experimentos jugando con los optimizers y el valor del learning rate.

Imaginemos que tenemos un dataset completo que queremos explotar, nuestra labor será coger este dataset (California Housing Dataset) y desde 0 intentar llegar conseguir un modelo que tenga un buen rendimiento ajustándolo poco a poco como hemos visto en clase. Por lo que tendremos que entrenar distintas redes y comparar los resultados que obtengamos en cada experimento para ver cual es mejor.

Cada experimento que tendremos que realizar estará bien definido, la red que deberéis crear y entrenar será proporcionada por lo que solamente tendréis que crear la red que se nos indica con TensorFlow y realizar el entrenamiento de la misma.

## <h1><font color="#113D68" size=5> **Fases del Proyecto**</font></h1>


Será obligatorio realizar cada uno de los ejercicios que están definidos. En cada ejercicio está definida la red que se tiene que crear y la configuración con la que se tiene que entrenar, por lo que solamente tendréis que pasar esa definición a código con TensorFlow.

Para tener una buena práctica en la realización de este Proyecto se ofrecen está recomendaciones:

Utiliza correctamente el sistema de celdas de jupyter. La libreta está realizada de tal forma que solo tendréis que completar las celdas que se indican, ya sea con código o con texto en markdown. Se recomienda rellenar solamente las celdas indicadas para que quede un informe limpio y fácil de seguir. Si fuera necesario incluir más celdas por cualquier motivo se puede hacer pero realizarlo con cuidado para no ensuciar demasiado la libreta.


Las redes que tendréis que crear en cada experimento son las vistas en clase, por lo que os podéis inspirar en los ejemplos vistos en los tutoriales. Os recomiendo que no copiéis y peguéis código tal cual, sino que lo escribáis por vuestra cuenta y entendáis lo que estáis haciendo en cada momento. Tomaros el tiempo que haga falta para entender cada paso.

Comprueba que todo se ejecuta correctamente antes de enviar tu trabajo. La mejor forma de enviarlo es exportando la libreta a pdf o html para enviarla en un formato más profesional.

Las fases que debes realizar son las siguientes:

    Fase 1: Familiarízate con todo el entorno de trabajo.
    Fase 2: Carga los datos y familiarízate con ellos.
    Fase 3: Realiza cada uno de los experimentos indicados.

#Fase 1: Familiarízate con todo el entorno de trabajo.

### Análisis del Dataset: California Housing

Esta vez vamos a utilizar un conjunto de datos que contienen información sobre el precio de las casas encontradas en un distrito de California. Las columnas son las siguientes:


#### **Descripción de las Variables**
- **Variables numéricas continuas:**
  - `longitude` y `latitude`: Coordenadas geográficas que pueden ser útiles para análisis espaciales.
  - `housingMedianAge`: Indicador de la antigüedad promedio de las viviendas.
  - `totalRooms`, `totalBedrooms`: Representan el tamaño total de las viviendas en el bloque.
  - `population`: Cantidad total de residentes en el bloque.
  - `households`: Número total de hogares (unidades residenciales) en el bloque.
  - `medianIncome`: Ingresos medianos por bloque, un predictor clave del valor de las viviendas.
  - `medianHouseValue`: Variable objetivo, el valor medio de las viviendas.

- **Variables categóricas:**
  - `oceanProximity`: Proximidad al océano, que seguramente se vea un impacto significativo en los precios de las viviendas.

---

#### **1. Propósito y Aplicaciones del Dataset**
Este Dataset se usara de la siguiente manera:

1) Predecir el promedio de las casas en un distrito (columna `medianHouseValue`).

2) Examinar cómo las características geográficas (`longitude` y `latitude`) afectan los precios de las viviendas.

3) Analizar relaciones entre ingresos, población y precios de viviendas.

---


#### **2. Exploración de Datos y Limpieza**

Antes de entrenar el modelo, es necesario hacer primero lo siguiente:

- **Explorar la distribución de las variables:**

  - Histograma para `medianHouseValue` (¿hay valores extremos?).

  - Distribución geográfica con `longitude` y `latitude` para observar clusters de precios.

- **Saber que hacer con valores nulos:**

  - Es común que `households` tenga valores nulos. Considerar estrategias como imputación basada en medianas o relaciones con otras variables (`totalRooms`, `totalBedrooms` ).

- **Transformaciones necesarias:**

  - Escalado de datos: Las columnas como `medianIncome` y `population` tienen rangos muy diferentes.

  - Características: Población por hogar seria un forma de deducirlo.

---

#### **3. Análisis Inicial**

Algunas preguntas exploratorias que se deben resolver apenas se ven los dato serian :

- **Relación entre proximidad al océano y precios:**

  - ¿Cómo afecta `oceanProximity` a `medianHouseValue`? Por su tipo podriamos usar gráficos de cajas para explorar esto.

- **Impacto de los ingresos en los precios de vivienda:**

  - Relación lineal entre `medianIncome` y `medianHouseValue`.

- **Densidad de población y calidad de vida:**

  - ¿Las áreas con alta densidad (`population/households`) tienen valores de vivienda más bajos?

---

#### **4. Modelos Propuestos**

Segun lo anterior este dataset es ideal para:

 **Redes Neuronales:**
   - Especialmente útil si se integran coordenadas espaciales, drops y otras transformaciones.

---

#Fase 2: Carga los datos y familiarízate con ellos.

En esta actividad vamos a seguir familiarizándonos con la herramienta *TensorFlow*, para ello vamos a utilizar un dataset donde tendréis que relizar todos los pasos (cargar datos, crear arquitectura de la red, etc.)  *TensorFlow*.

Usando los datos visto en los ejercicios sobre casa y sus precios realiza los ejercicios que se indican.

La información de los datos podéis verla en este enlace:

https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data
"""

!pip install numpy scipy matplotlib
!pip install -U scikit-learn
!pip install altair
!pip install vega_datasets
!pip install tensorflow
!pip install scipy
!ls -l
!pip install tensorflow

"""### Importacion de librerias que vamos a utilizar

"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import altair as alt
from vega_datasets import data
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
from scipy.signal import savgol_filter
import numpy as np
import pandas as pd
from keras.src.api_export import keras_export
from keras.src.utils.file_utils import get_file
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Para mostrar gráficas
import matplotlib.pyplot as plt
# %matplotlib inline

# Anaconda fixing problem
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'

"""### Importamos el dataset para el practico"""

df = pd.read_csv('housing.csv')
print(df.info())

"""Se leen los datos y se realiza el analisis descriptivo"""

# Mostrar las primeras 10 filas del DataFrame para inspección
print("Primeras 10 filas del DataFrame:")
print(df.head(10))

# Mostrar estadísticas descriptivas del DataFrame
print("\nEstadísticas descriptivas del DataFrame:")
print(df.describe())


unique_classes = np.unique(df)  # Use the scaled data for unique values
print("\nClases únicas en el dataset:", unique_classes)

"""#`Copia de datos para tener un backup en caso de crisis`"""

import copy # imports the copy module
datos2 = copy.deepcopy(df)   # Crea una copia independiente
datos3 = copy.deepcopy(df)   # Crea una copia independiente

"""#Analisis descriptivo y exploratorio

"""

#descripcion estadisticas basicas
df.describe().T

# valores unicos para cada variable
df.agg(['nunique']).T

x = df[['longitude', 'latitude', 'housing_median_age', 'total_rooms',
       'total_bedrooms', 'population', 'households', 'median_income',
       'ocean_proximity']].values
y = df[['median_house_value']].values

"""### Análisis Descriptivo del Dataset de California Housing
El analisis descriptivo, sirve para mostrarnos las tendencias generales y las posibles áreas de mejora, asi vemos las consas a mejorar para futuros análisis, en especifico de Machine Learning.

---

#### **1. Estadísticas Generales**

El dataset consta de **20,433 registros** que representan los distritos de California y su ubicacion.

**Resumen de estadísticas principales:**

| Variable               | Media        | Desviación Estándar | Mínimo     | Percentil 25 | Mediana     | Percentil 75 | Máximo      |
|------------------------|--------------|---------------------|------------|--------------|-------------|--------------|-------------|
| Longitude              | -119.57      | 2.00                | -124.35    | -121.80      | -118.49     | -118.01      | -114.31     |
| Latitude               | 35.63        | 2.14                | 32.54      | 33.93        | 34.26       | 37.72        | 41.95       |
| Housing Median Age     | 28.63        | 12.59               | 1.00       | 18.00        | 29.00       | 37.00        | 52.00       |
| Total Rooms            | 2636.50      | 2185.27             | 2.00       | 1450.00      | 2127.00     | 3143.00      | 39320.00    |
| Total Bedrooms         | 537.87       | 421.39              | 1.00       | 296.00       | 435.00      | 647.00       | 6445.00     |
| Population             | 1424.95      | 1133.21             | 3.00       | 787.00       | 1166.00     | 1722.00      | 35682.00    |
| Households             | 499.43       | 382.30              | 1.00       | 280.00       | 409.00      | 604.00       | 6082.00     |
| Median Income          | 3.87         | 1.90                | 0.50       | 2.56         | 3.54        | 4.74         | 15.00       |
| Median House Value     | 206,864.41   | 115,435.67          | 14,999.00  | 119,500.00   | 179,700.00  | 264,700.00   | 500,001.00  |
| Ocean Proximity (categorical)| -         | -                   | 0          | 0            | 1           | 1            | 4           |

---

#### **2. Observaciones Clave**
1. **Geografía (`longitude`, `latitude`):**

   - Coordenadas del estado de California.

     - Oeste: `longitude` hasta -124.35.
     - Norte: `latitude` hasta 41.95.

2. **Edad promedio de las viviendas (`housing_median_age`):**

   - La edad promedio es **28.63 años**, desvisacion estandar **12.59 años**
   - La concentración principal posible forma de campana está entre los **18 y 37 años** (percentiles 25 y 75).

3. **Variables de capacidad (`total_rooms`, `total_bedrooms`, `households`, `population`):**

   - Existe una gran dispersión en estas variables.
   Ejemplo: `total_rooms` tiene un rango de 2 a 39,320, mientras que `total_bedrooms`.

   - La densidad poblacional (calculada como `population/households`) podría ser una métrica interesante para analizar.

4. **Ingresos y valores de viviendas (`median_income`, `median_house_value`):**

   - Los ingresos medianos entre 2.56 y 4.74 (percentiles 25 y 75), con un máximo de 15.00.


5. **Proximidad al océano (`ocean_proximity`):**

   - Clasificación categórica con 5 niveles únicos: valores 0 (No océano) a 4 (muy cerca del océano).

   - Podriamos considerar que esta variable influye en el precio de las viviendas.

---

#### **3. Observaciones de Valores Extremos**


- **`median_house_value`:** (500,001)

- **`total_rooms`, `population`:** 39,320 habitaciones y 35,682 personas son datos que definitivamente distorsionan las métricas.

- **`median_income`:** El ingreso máximo (15.00) sabiendo que la media de 3.87.

---

#### **4. Variables Categóricas**

- **`ocean_proximity`:** Esta variable tiene 5 categorías que es nesesario tranformarlas con One-Hot Encoding para ser usada en modelos.

---

#### **5. Oportunidades para Ingeniería de Características**

- **Densidad de ocupación:**

  - `population / households` para medir la densidad poblacional por hogar.
- **Densidad de habitaciones:**

  - `total_rooms / households` o `total_bedrooms / households` para analizar el tamaño promedio de viviendas.

- **Distancia al océano:**
  - Convertir `ocean_proximity` en una métrica ordinal si es aplicable (e.g., lejos = 0, cerca = 4).

---

#### **6. Próximos Pasos**

- **Visualización:**
  - Crear mapas de calor geográficos basados en `longitude`, `latitude` y `median_house_value`.

  - Gráficos de matrices de correlación para explorar relaciones entre ingresos, población y valores de vivienda.

- **Preprocesamiento:**

  - Manejo de valores extremos.
  - Imputación de valores faltantes en `total_bedrooms` si se detectan ausencias.

  - Escalado de variables como ingresos y valores de viviendas.

#Matriz de Correlaciones
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np # Import the NumPy library

plt.figure(figsize=(20, 18))  # Tamaño de la figura

# Seleccionar solo columnas numéricas excluyendo las columnas especificadas
numeric_cols = df.select_dtypes(include=np.number).columns

# Calcular la matriz de correlación
corr = df[numeric_cols].corr()

# Crear el mapa de calor utilizando Seaborn con esquema de color personalizado
sns.heatmap(corr, annot=True, cmap='Blues', linewidths=0.5, fmt='.2f',
            vmin=-1, vmax=1)  # vmin y vmax para establecer el rango de correlación

# Ajustar el título y mostrar el gráfico
plt.title('Mapa de calor de correlación')

# Mostrar el gráfico
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
# Diagrama de pares para visualizar las relaciones en el marco de datos:
sns.pairplot(df, palette= "Blues_d") # Se pone una paleta de colores azul
plt.show()

from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# Verificar y preparar los datos
target_column = 'median_house_value'
features = df.drop(columns=[target_column, 'ocean_proximity'], errors='ignore')  # Eliminamos la columna categórica
labels = df[target_column]

# Convertir la columna objetivo en categorías para clasificación
labels = pd.qcut(labels, q=4, labels=False)  # Dividir en 4 cuartiles como ejemplo

# Crear y entrenar el modelo Random Forest
rf = RandomForestClassifier(n_estimators=150, random_state=42)
rf.fit(features, labels)

# Obtener las importancias de las características
feature_importances = rf.feature_importances_
features_df = pd.DataFrame({
    'Característica': features.columns,
    'Importancia': feature_importances
}).sort_values(by='Importancia', ascending=False)

# Mostrar las características más importantes
features_df

# Graficar la importancia de las características
plt.figure(figsize=(10, 6))
plt.barh(features_df['Característica'], features_df['Importancia'], color='b')
plt.xlabel('Importancia')
plt.ylabel('Características')
plt.title('Importancia de las Variables - Random Forest')
plt.gca().invert_yaxis()  # Invierte el eje Y para mostrar las más importantes primero
plt.tight_layout()
plt.show()

"""# Fase 3"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
# Codificar la columna categórica 'ocean_proximity' con OneHotEncoder
encoder = OneHotEncoder(sparse_output=False, drop='first')
x_categorical = encoder.fit_transform(x[:, [-1]])  # La última columna es 'ocean_proximity'

# Concatenar las columnas codificadas con las demás características
x = np.hstack((x[:, :-1], x_categorical))  # Excluye la última columna y añade las codificadas

# Dividir los datos en conjuntos de entrenamiento y prueba
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Escalar las características
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

"""## <font color="#004D7F" size=4>Ejercicio 2</font>
Crear una red con la siguiente configuración y entrénala:

- **Configuración de la red**:
    - Arquitectura de la red:
        - *1º Capa*: capa de entrada donde indiques la dimensión de los datos.
        - *2º Capa*: capa densa con 8 neuronas y función de activación *relu*.
        - *3º Capa*: capa densa con 8 neuronas y función de activación *relu*.
        - *4º Capa*: capa de salida con una neurona sin función de activación.
    - Tipo de entrenamiento:
        - *Epochs*: 100
        - *Optimizador*: *adam*
        - *Learning Rate*: 0.001
        - *Función de error*: error cuadrático medio (*mean_squared_error*)
"""

# Crear el modelo secuencial
model = Sequential()

# Añadir capas con la configuración proporcionada
# 1º Capa: capa de entrada (input_dim indica la dimensión de entrada)
model.add(Dense(8, activation='relu', input_dim=x_train.shape[1]))

# 2º Capa: capa densa con 8 neuronas y activación relu
model.add(Dense(8, activation='relu'))

# 3º Capa: capa densa con 8 neuronas y activación relu
model.add(Dense(8, activation='relu'))

# 4º Capa: capa de salida con una neurona (sin función de activación)
model.add(Dense(1))

# Configuración del optimizador con un learning rate de 0.001
opt_1 = tf.keras.optimizers.Adam(learning_rate=0.001)  # Optimizador Adam


model.compile(optimizer=opt_1,
                 loss='mean_absolute_percentage_error',
                 metrics=['mean_absolute_error'])     # Métrica MAE

# Entrenamiento del modelo
history = model.fit(
    x_train,
    y_train,
    epochs=100,
    validation_data=(x_test, y_test),
    verbose=1
)

# Evaluar el modelo en el conjunto de entrenamiento
loss, mae = model.evaluate(x_train, y_train)
print(f"Pérdida (MAPE): {loss:.4f}, Error absoluto medio (MAE): {mae:.2f}")

# Evaluación del modelo en el conjunto de prueba
loss = model.evaluate(x_test, y_test, verbose=1)
print(f"\nError cuadrático medio en el conjunto de prueba 1 : {loss}")

"""#### **Métricas obtenidas Modelo 1:**
- **Conjunto de entrenamiento:**
  - **`loss (MSE)`**: 22.1036
  - **`MAE`**: 46,645.23

- **Conjunto de validación:**
  - **`val_loss (MSE)`**: 21.9447
  - **`val_MAE`**: 47,810.17

- **Conjunto de prueba:**
  - **`loss (MSE)`**: 21.9447
  - **`MAE`**: 47,810.17
  - **`MAPE`**: 21.9006%

- **Evaluación adicional:**
  - **`MAE`**: 46,788.74
  - **`loss (MSE)`**: 21.7334

---

#### ** Interpretación de los resultados:**

1. **Errores absolutos (MAE):**
   - El **MAE** de ~46,000-47,800 unidades este valor puede ser razonable o alto dependiendo del rango de la variable objetivo.

2. **Error relativo (MAPE):**
   - El **MAPE** de ~21.90% indica que las predicciones están desviadas el desempeño.

3. **Consistencia de las métricas:**

   Gracias a los grupos de validacion y entrenamiento no hay indicios claros de sobreajuste (overfitting) ni subajuste (underfitting).

4. **Magnitud del MSE:**

   - El **MSE** es una métrica que penaliza más los errores grandes, su similitud entre el entrenamiento y el de validacion confirma una buena generalización.

El modelo tiene un buen **MAE** consistente entre grupos (~46,000-47,800) y un **MAPE** que es bueno (~21.9%). Tendriamos que buscar la manera de reducir en MAPE, puede mejorarse. Quiza ajustando los de hiperparámetros, preprocesamiento y análisis de errores residuales puede optimizar el desempeño.
"""

import matplotlib.pyplot as plt

# Crear un gráfico con dos ejes y escalas diferentes
fig, ax1 = plt.subplots(figsize=(12, 6))

# Eje izquierdo para MAE
ax1.plot(history.history['mean_absolute_error'], label='Train MAE', linestyle='-', color='darkblue')
ax1.set_xlabel('Époch')
ax1.set_ylabel('MAE', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# Eje derecho para MAPE
ax2 = ax1.twinx()
ax2.plot(history.history['loss'], label='Train MAPE (Loss)', linestyle='-', color='orange')
ax2.set_ylabel('MAPE', color='orange')
ax2.tick_params(axis='y', labelcolor='orange')

# Títulos y leyendas
plt.title('Evolución del MAE y MAPE durante el Entrenamiento Modelo 1')
fig.tight_layout()
plt.show()

"""## <font color="#004D7F" size=4>Ejercicio 3</font>
Crear una red con la siguiente configuración y entrénala:

- **Configuración de la red**:
    - Arquitectura de la red:
        - *1º Capa*: capa de entrada donde indiques la dimensión de los datos.
        - *2º Capa*: capa densa con 16 neuronas y función de activación *relu*.
        - *3º Capa*: capa densa con 16 neuronas y función de activación *relu*.
        - *4º Capa*: capa densa con 32 neuronas y función de activación *relu*.
        - *4º Capa*: capa de salida con una neurona sin función de activación.   
    - Tipo de entrenamiento:
        - *Epochs*: 300
        - *Optimizador*: *adam*
        - *Learning Rate*: 0.0001
        - *Función de error*: error cuadrático medio (*mean_squared_error*)
"""

# Crear el modelo secuencial
model2 = Sequential()

# Añadir capas
# 1º Capa: capa de entrada (input_dim indica la dimensión de entrada)
model2.add(Dense(16, activation='relu', input_dim=x_train.shape[1]))

# 2º Capa: capa densa con 16 neuronas y activación relu
model2.add(Dense(16, activation='relu'))

# 3º Capa: capa densa con 32 neuronas y activación relu
model2.add(Dense(32, activation='relu'))

# 4º Capa: capa de salida con una neurona (sin función de activación)
model2.add(Dense(1))

# Configuración del optimizador con un learning rate de 0.0001
optimizer2 = Adam(learning_rate=0.0001)

# Compilación del modelo
model2.compile(optimizer=optimizer2,
                 loss='mean_absolute_percentage_error',
                 metrics=['mean_absolute_error'])     # Métrica MAE

# Entrenamiento del modelo
history2 = model2.fit(
    x_train,
    y_train,
    epochs=300,
    validation_data=(x_test, y_test),
    verbose=1
)


# Evaluar el modelo en el conjunto de entrenamiento
loss, mae1 = model.evaluate(x_train, y_train) # This is evaluating model, not model2. consider changing it to model2
print(f"Pérdida (MAPE): {loss:.4f}, Error absoluto medio (MAE): {mae1:.2f}")

# Evaluación del modelo en el conjunto de prueba
loss2 = model2.evaluate(x_test, y_test, verbose=1)
print(f"\nError cuadrático medio en el conjunto de prueba 2 : {loss2}")

""" Métricas de rendimiento modelo 2 observadas:

    Entrenamiento:
        loss (MSE): 22.3766
        MAE: 48520.0000

    Validación:
        val_loss (MSE): 22.4240
        val_MAE: 49297.6523

    Prueba adicional:
        loss (MSE): 21.8123
        MAE: 45860.1719
        MAPE: 21.9006
        Error cuadrático medio adicional: 22.4239

2. Análisis del modelo:

El modelo tiene un error absoluto medio (MAE) de 46,000-49,000 unidades en los diferentes conjuntos. El MAPE (~21.9%) indica que, en promedio, las predicciones están desviadas un ~22% de los valores reales, que segun el contexto podriamos decir que es alto.

Las métricas se mantienen en ambos conjuntos tanto los de entrenamiento,para validación y prueba. Esto sugiere que el modelo está generalizando bien y es dificil saber si tiene un sobreajuste o subajuste.

El MAE de 46,000-49,000 unidades como la variable objetivo está en cientos de miles, el error es tolerable, dado que es como un promedio.

MAPE: Un error de 22%, tiene un desempeño razonable.
"""

import matplotlib.pyplot as plt

# Crear un gráfico con dos ejes y escalas diferentes
fig, ax1 = plt.subplots(figsize=(12, 6))

# Eje izquierdo para MAE
ax1.plot(history2.history['mean_absolute_error'], label='Train MAE', linestyle='-', color='darkblue')
ax1.set_xlabel('Époch')
ax1.set_ylabel('MAE', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# Eje derecho para MAPE
ax2 = ax1.twinx()
ax2.plot(history2.history['loss'], label='Train MAPE (Loss)', linestyle='-', color='orange')
ax2.set_ylabel('MAPE', color='orange')
ax2.tick_params(axis='y', labelcolor='orange')

# Títulos y leyendas
plt.title('Evolución del MAE y MAPE durante el Entrenamiento Modelo 2')
fig.tight_layout()
plt.show()

"""### <font color="#004D7F" size=4>Ejercicio 4</font>
Crear una red con la siguiente configuración y entrénala:

- **Configuración de la red**:
    - Arquitectura de la red:
        - *1º Capa*: capa de entrada donde indiques la dimensión de los datos.
        - *2º Capa*: capa densa con 32 neuronas y función de activación *relu*.
        - *3º Capa*: capa densa con 64 neuronas y función de activación *relu*.
        - *4º Capa*: capa densa con 128 neuronas y función de activación *relu*.
        - *4º Capa*: capa de salida con una neurona sin función de activación.   
    - Tipo de entrenamiento:
        - *Epochs*: 2OO
        - *Optimizador*: *adam*
        - *Learning Rate*: 0.001
        - *Función de error*: error cuadrático medio (*mean_squared_error*)
"""

# Crear el modelo secuencial
model3 = Sequential()

# Añadir capas según la nueva configuración proporcionada
# 1º Capa: capa de entrada (input_dim indica la dimensión de entrada)
model3.add(Dense(32, activation='relu', input_dim=x_train.shape[1]))

# 2º Capa: capa densa con 64 neuronas y activación relu
model3.add(Dense(64, activation='relu'))

# 3º Capa: capa densa con 128 neuronas y activación relu
model3.add(Dense(128, activation='relu'))

# 4º Capa: capa de salida con una neurona (sin función de activación)
model3.add(Dense(1))

# Configuración del optimizador con un learning rate de 0.001
optimizer3 = Adam(learning_rate=0.001)

# Compilación del modelo
model3.compile(optimizer=optimizer3,
                 loss='mean_absolute_percentage_error',
                 metrics=['mean_absolute_error'])     # Métrica MAE


# Entrenamiento del modelo
history3 = model3.fit(
    x_train,
    y_train,
    epochs=200,
    validation_data=(x_test, y_test),
    verbose=1
)

# Evaluar el modelo en el conjunto de entrenamiento
loss3, mae3 = model3.evaluate(x_train, y_train)
print(f"Pérdida (MAPE): {loss3:.4f}, Error absoluto medio (MAE): {mae3:.2f}")

# Evaluación del modelo en el conjunto de prueba
loss3 = model3.evaluate(x_test, y_test, verbose=1)
print(f"\nError cuadrático medio en el conjunto de prueba 2 : {loss3}")

"""Datos del modelo 3 :

    Entrenamiento:
        loss (MSE): 19.8601
        MAE: 42665.4570

    Prueba:
        loss (MSE): 20.1152
        MAE: 43464.4531
        MAPE: 19.6958

Este modelo genera un (MAE) de ~42,000-43,000 unidades en los conjuntos de entrenamiento y prueba, con un error relativo (MAPE) del 19.7%.

Los nos muestran que los conjuntos de prueba y aprendizaje son similares entre ambos conjuntos lo que nos podria decir que es una buena generalizacion de los datos, y no existe sobreajuste aparente gracias a que se entrena y se prueban con datos distintos. Aunque el error absoluto es alto para el contexto, podriamos explorar que datos podrian ser tranformados o que hiperparámetros se deben tener ademas de mirar que valores atípicos se deben eliminar para mejorar el rendimiento. La métrica MAPE indica un desempeño razonable, aunque podría requerir ajustes para mayor precisión.
"""

import matplotlib.pyplot as plt

# Crear un gráfico con dos ejes y escalas diferentes
fig, ax1 = plt.subplots(figsize=(12, 6))

# Eje izquierdo para MAE
ax1.plot(history3.history['mean_absolute_error'], label='Train MAE', linestyle='-', color='darkblue')
ax1.set_xlabel('Époch')
ax1.set_ylabel('MAE', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# Eje derecho para MAPE
ax2 = ax1.twinx()
ax2.plot(history3.history['loss'], label='Train MAPE (Loss)', linestyle='-', color='orange')
ax2.set_ylabel('MAPE', color='orange')
ax2.tick_params(axis='y', labelcolor='orange')

# Títulos y leyendas
plt.title('Evolución del MAE y MAPE durante el Entrenamiento Modelo 3')
fig.tight_layout()
plt.show()

"""## <font color="#004D7F" size=4>Ejercicio 5</font>
Compara los resultados obtenidos en cada uno de los modelos entrenados y quédate con el mejor. **Justifica tu respuesta**.


### Análisis optimizado de los modelos entrenados  

Para este analisis se hace una comparativa de los distintos modelos 1, 2 y 3, se usan las métricas clave, para elegir el modelo más adecuado y justificar la selección, el rendimiento global.

---

#### **Comparación de los resultados**  

| **Métrica**  | **Modelo 1**  ++      | **Modelo 2**    +    | **Modelo 3**     +++   |
|---------------|---------------------|---------------------|---------------------|
| **MSE (Prueba)** | 20.3541 ++            | 22,4113     +       | **20.1152** +++        |
| **MAE (Prueba)** | 44027.35 ++         | 48961.90     +     | **43464.45**  +++    |
| **MAPE (Prueba)**| 21.9006  ++          | 21900.6   +          | **19.6958**    +++    |

#### **Observaciones clave**  

1. **MSE (Error cuadrático medio):**
   - El **Modelo 3** tiene el menor MSE con (**20.1152**), a priori es la mejor prediccion, minimizando los errores elevados más eficientemente.

2. **MAE (Error absoluto medio):**
   - El **Modelo 3** también en el MAE (**43,464.45**), se puede ver que es el mejor.

3. **MAPE (Error porcentual):**
   - El **Modelo 3** tiene el menor MAPE (**19.6958%**), porcentual en relación con los valores reales.

4. **Generalización:**
   - El Modelo 3 tiene buena consistencia entre los conjuntos de entrenamiento y prueba (MSE y MAE similares), diciendonos que no esta sobreajustado ni subajustado. Esto contrasta ligeramente con los Modelos 1 y 2, donde los valores son más variables.

---

### **Elección del mejor modelo**

**Modelo seleccionado: Modelo 3**  

1. **Mejor desempeño global:** El Modelo 3 es el mas destacado de las tres métricas clave (MSE, MAE y MAPE) frente a los otros modelos.

2. **Generalización sólida:** Las métricas del conjunto de prueba y entrenamiento similares y muestran un desempeño bueno lo que indica que el modelo no depende excesivamente de los datos de entrenamiento.

3. **Menor error porcentual (MAPE):** El error relativo más bajo (19.7%) asegura que el modelo sea más confiable en diferentes contextos, especialmente si el rango de la variable objetivo es grande.

---

### **Conclusión**


El **Modelo 3** es la mejor opción debido a su balance superior entre las métricas principales (MSE, MAE y MAPE) y su capacidad de generalización. Con ajustes adicionales en el preprocesamiento y la configuración del modelo, es posible mejorar aún más su desempeño, particularmente en términos de precisión porcentual (MAPE).

Si el objetivo principal fuera minimizar el error absoluto (MAE) o un error porcentual relativo (MAPE), podríamos revaluar los modelos en función de ese criterio. Sin embargo, bajo los resultados actuales, el Modelo 1 es la mejor elección.

---


"""

# Graficamos la pérdida de entrenamiento y validación de los 3 modelos para compararlos

plt.plot(history.history['loss'], label='Pérdida de train modelo 1', color='red')
plt.plot(history.history['val_loss'], label='Pérdida de test modelo 1', color='orange')
plt.plot(history2.history['loss'], label='Pérdida de train modelo 2', color='green')
plt.plot(history2.history['val_loss'], label='Pérdida de test modelo 2', color='blue')
plt.plot(history3.history['loss'], label='Pérdida de train modelo 3',color='black')
plt.plot(history3.history['val_loss'], label='Pérdida de test modelo 3',color='grey')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.title('Curvas de aprendizaje')
plt.ylim(0, 200)
plt.show()

# Completar
model3.save('Mejor_modelo.h5')

"""## <font color="#004D7F" size=4>Ejercicio 6 (Opcional)</font>
Carga el mejor modelo que has guardado en el ejercicio anterior usando la función load_model y realizar predicciones usando la función predict.

Para realizar las predicciones usa 10 datos cualquier del conjunto de test:
"""

# Crear el modelo secuencial
model3 = Sequential()

# Añadir capas según la nueva configuración proporcionada
# 1º Capa: capa de entrada (input_dim indica la dimensión de entrada)
model3.add(Dense(32, activation='relu', input_dim=x_train.shape[1]))

# 2º Capa: capa densa con 64 neuronas y activación relu
model3.add(Dense(64, activation='relu'))

# 3º Capa: capa densa con 128 neuronas y activación relu
model3.add(Dense(128, activation='relu'))

# 4º Capa: capa de salida con una neurona (sin función de activación)
model3.add(Dense(1))

# Configuración del optimizador con un learning rate de 0.001


# Compilación del modelo
model3.compile(optimizer=optimizer3,
                 loss='',
                 metrics=['mean_absolute_error'])     # Métrica MAE


# Entrenamiento del modelo
history3 = model3.fit(
    x_train,
    y_train,
    epochs=200,
    validation_data=(x_test, y_test),
    verbose=1
)

# Evaluar el modelo en el conjunto de entrenamiento
loss3, mae3 = model3.evaluate(x_train, y_train)
print(f"Pérdida (MAPE): {loss3:.4f}, Error absoluto medio (MAE): {mae3:.2f}")

# Completar
from tensorflow.keras.models import load_model

# Cargamos el modelo guardado
Mejor_modelo = load_model('Mejor_modelo.h5')

optimizer4 = Adam(learning_rate=0.0001)

# Compilamos el modelo para incluir las métricas, aunque tampoco sería un paso necesario
Mejor_modelo.compile(optimizer=optimizer4, loss='mean_absolute_percentage_error', metrics=['mean_squared_error'])

# Evaluaamos el modelo para comprobar que nos da las métricas correctas que hemos obtenido en el model3
evaluation = Mejor_modelo.evaluate(x_test, y_test)
print("Evaluación en el conjunto de test:", evaluation)

# Realizamos las predicciones
predictions = Mejor_modelo.predict(x_test[:25])
print("Predicciones para 25 muestras del conjunto de test:")
print(predictions)

"""#Sprint 3

## 1. Establecer una función de coste adecuada a nuestro problema.

En este caso, como es un problema de regresión y los valores de nuestros datos son no tan grandes, elegimos la función de coste `mean_absolute_percentage_error`, este error varía entre los valores 100 y 0 donde 100 es el pero error que podemos llegar a tener y 0 es el mejor error, por lo que en nuestros entrenamientos buscaremos un error más cercano a 0.
"""

actual_loss = 'mean_absolute_percentage_error'

"""## 2. Elegir una estructura de red base

Ahora, como ya hemos visto en clase vamos a encontrar una estructura de red que encaje con los datos que vamos a utilizar. Vamos a crear varias redes a ver que talfuncionan.

Para hacer entrenamientos rápidos y ver si la red se adapta a los datos vamos a usar solo un subconjunto de los datos, es decir usaremos 1000 datos y no usaremos conjunto de validación.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Se tiene presente que la variable 'ocean_proximity' es categorica y que se realiza con OneHotEncoder
Codificador= OneHotEncoder(sparse_output=False, drop='first')
x_Cat = Codificador.fit_transform(x[:, [-1]])  # La última columna es 'ocean_proximity'

# Concatenar las columnas codificadas con las demás características
x = np.hstack((x[:, :-1], x_Cat))  # Excluye la última columna y añade las codificadas

# Dividir los datos en conjuntos de entrenamiento y prueba
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Escalar las características
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Reducir el tamaño del conjunto de entrenamiento a 1,000 muestras para pruebas rápidas
x_Quitotrain = x_train[:1000]
y_Quitotrain = y_train[:1000]

"""## <font color="#004D7F" size=4>Ejercicio 1</font>

Crear una red con la siguiente configuración y entrénala:

- **Configuración de la red**:
    - Arquitectura de la red:
        - *1º Capa*: capa de entrada donde indiques la dimensión de los datos.
        - *2º Capa*: capa densa con 8 neuronas y función de activación *relu*.
        - *3º Capa*: capa densa con 8 neuronas y función de activación *relu*.
        - *4º Capa*: capa de salida con una neurona sin función de activación.
        
    - Tipo de entrenamiento:
        - *Epochs*: 30
        - *Optimizador*: *adam*
        - *Learning Rate*: 0.001
"""

#crear el modelo
model1 = Sequential()

# Añadimos las capas
#  1 Capa
model1.add(Dense(9, activation='relu', input_dim=x_Quitotrain.shape[1])) # capa entrada
model1.add(Dense(8, activation='relu')) # segunda capa 8
model1.add(Dense(8, activation='relu')) # tercera capa 8
model1.add((Dense(1)))  # capa salida sin activacion

#configuracion del Optimizador
optimizador = Adam(learning_rate=0.001)

# Compilar el modelo antes de entrenarlo
model1.compile(loss=actual_loss, optimizer=optimizador, metrics=['mae']) # Compilar

# Entrenamiento del modelo
Prueba_1 = model1.fit(
    x_Quitotrain, #datos de entrada
    y_Quitotrain, #datos de salida
    epochs=30,
    batch_size=35,
    verbose=1,
    validation_data=(x_test, y_test)  # Validacion
)

# Visualización opcional
plt.plot(Prueba_1.history['loss'], label='Pérdida Entrenamiento')
plt.plot(Prueba_1.history['val_loss'], label='Pérdida Validación')  # Now 'val_loss' should be available
plt.xlabel('Épocas')
plt.ylabel('Pérdida')
plt.legend()
plt.title('Curva de Pérdida')
plt.show()

"""### **Análisis del Resultado del Entrenamiento**

#### ***Metricas Modelo 1***

- **`loss` (Pérdida en entrenamiento):** 96.3094 (mean_absolute_percentage_error)
- **`mae` (Error absoluto medio en entrenamiento):** 202,316.22  
- **`val_loss` (Pérdida en validación):** 95.8981  (mean_absolute_percentage_error)  
- **`val_mae` (Error absoluto medio en validación):** 200,796.44  


Se puede ver que existe consistencia entre entrenamiento y la validación, los valores de pérdida (`loss` vs `val_loss`) y MAE (`mae` vs `val_mae`) son muy similares dado a que son distintos conjunto se podria decir que no hay  **sobreajuste**, ni de **subajuste** .  

**Magnitud de los errores:**  

   - Los valores absolutos del MAE (**200,000**) promedio significativo en las predicciones.

### **Conclusión**  

El modelo muestra buena generalización entre los conjuntos de entrenamiento y validación, pero el error absoluto promedio (MAE ~200,000) es alto, y la pérdida (MSE) sugiere la presencia de errores grandes. Para mejorar:

### <font color="#004D7F" size=4>Ejercicio 2</font>

Vamos a complicar un poco más la arquitectura de la red:

- **Configuración de la red**:
    - Arquitectura de la red:
        - *1º Capa*: capa de entrada donde indiques la dimensión de los datos.
        - *2º Capa*: capa densa con 64 neuronas y función de activación *relu*.
        - *3º Capa*: capa densa con 32 neuronas y función de activación *relu*.
        - *4º Capa*: capa densa con 32 neuronas y función de activación *relu*.
        - *5º Capa*: capa de salida con una neurona sin función de activación.
        
    - Tipo de entrenamiento:
        - *Epochs*: 30
        - *Optimizador*: *adam*
        - *Learning Rate*: 0.001
"""

#crear el modelo
model2 = Sequential()

# Añadimos las capas
#  1 Capa
model2.add(Dense(9, activation='relu', input_dim=x_Quitotrain.shape[1])) # capa entrada
model2.add(Dense(64, activation='relu')) # segunda capa 64
model2.add(Dense(32, activation='relu')) # tercera capa 32
model2.add(Dense(32, activation='relu')) # cuarta capa 32
model2.add((Dense(1)))  # capa salida sin activacion

#configuracion del Optimizador
optimizador2 = Adam(learning_rate=0.001)

# Compilar el modelo antes de entrenarlo
model2.compile(loss=actual_loss, optimizer=optimizador2, metrics=['mae']) # Compilar

# Entrenamiento del modelo
Prueba_2 = model2.fit(
    x_Quitotrain, #datos de entrada
    y_Quitotrain, #datos de salida
    epochs=30,
    batch_size=35,
    verbose=1,
    validation_data=(x_test, y_test)  # Validacion
)

# Visualización opcional
plt.plot(Prueba_2.history['loss'], label='Pérdida Entrenamiento')
plt.plot(Prueba_2.history['val_loss'], label='Pérdida Validación')  # Now 'val_loss' should be available
plt.xlabel('Épocas')
plt.ylabel('Pérdida')
plt.legend()
plt.title('Curva de Pérdida')
plt.show()

"""###***Modelo 2***

El modelo de deep learning ha completado 30 épocas de entrenamiento. Las métricas clave reportadas son las siguientes:


- **`loss` (Pérdida en entrenamiento):** 33.5313 (mean_absolute_percentage_error).
- **`mae` (Error absoluto medio en entrenamiento):** 58,774.62.
- **`val_loss` (Pérdida en validación):** 34.1637 (mean_absolute_percentage_error).
- **`val_mae` (Error absoluto medio en validación):** 59,747.57  


El mean_absolute_percentage_error 33.5313 y un MAE de 58,774 en el conjunto de entrenamiento. En validación, los valores son similares: 34.1637 mean_absolute_percentage_error  para la pérdida y 59,747 para el MAE, indicando una buena generalización sin signos de sobreajuste ni subajuste.

El MAE promedio de ~59,000 unidades sugiere una desviación considerable en las predicciones. Esto podría deberse a características no normalizadas, valores atípicos o una arquitectura de modelo que no captura completamente la complejidad del problema.

## <font color="#004D7F" size=4>Ejercicio 3</font>

Vamos a complicar aun más la arquitectura de la red:

- **Configuración de la red**:
    - Arquitectura de la red:
        - *1º Capa*: capa de entrada donde indiques la dimensión de los datos.
        - *2º Capa*: capa densa con 128 neuronas y función de activación *relu*.
        - *3º Capa*: capa densa con 64 neuronas y función de activación *relu*.
        - *4º Capa*: capa densa con 32 neuronas y función de activación *relu*.
        - *5º Capa*: capa densa con 16 neuronas y función de activación *relu*.
        - *6º Capa*: capa de salida con una neurona sin función de activación.
        
    - Tipo de entrenamiento:
        - *Epochs*: 30
        - *Optimizador*: *adam*
        - *Learning Rate*: 0.001
"""

#crear el modelo
model_3 = Sequential()

# Añadimos las capas
#  1 Capa
model_3.add(Dense(9, activation='relu', input_dim=x_Quitotrain.shape[1])) # capa entrada
model_3.add(Dense(128, activation='relu')) # segunda capa 128
model_3.add(Dense(64, activation='relu')) # tercera capa 64
model_3.add(Dense(32, activation='relu')) # cuarta capa 32
model_3.add(Dense(16, activation='relu')) # cuarta capa 16
model_3.add((Dense(1)))  # capa salida sin activacion

#configuracion del Optimizador
optimizador_3 = Adam(learning_rate=0.001)

# Compilar el modelo antes de entrenarlo
model_3.compile(loss=actual_loss, optimizer=optimizador_3, metrics=['mae']) # Compilar

# Entrenamiento del modelo
Prueba_3 = model_3.fit(
    x_Quitotrain, #datos de entrada
    y_Quitotrain, #datos de salida
    epochs=30,
    batch_size=35,
    verbose=1,
    validation_data=(x_test, y_test)  # Validacion
)

# Visualización opcional
plt.plot(Prueba_3.history['loss'], label='Pérdida Entrenamiento')
plt.plot(Prueba_3.history['val_loss'], label='Pérdida Validación')
plt.xlabel('Épocas')
plt.ylabel('Pérdida')
plt.legend()
plt.title('Curva de Pérdida')
plt.show()

"""### **Modelo 3**

El modelo de deep learning ha completado 30 épocas de entrenamiento. Las métricas reportadas son:  

- **`loss` (Pérdida en entrenamiento):** 25.3974.  
- **`mae` (Error absoluto medio en entrenamiento):** 49,985.11.  
- **`val_loss` (Pérdida en validación):** 25.8387.  
- **`val_mae` (Error absoluto medio en validación):** 55,148.70.  

**Generalización del modelo:**

   - Las métricas de validación (`val_loss` y `val_mae`) son ligeramente peores que las de entrenamiento, pero están dentro de un rango aceptable, lo que sugiere que el modelo tiene buena generalización sin signos de sobreajuste evidente.

**Magnitud del error:**

   - El **MAE** promedio de 50,000-55,000 indica que el modelo tiene una desviación considerable respecto insuficiente capacidad del modelo.


"""

# Resumen del modelo
print("\nResumen del modelo:")
model_3.summary()

"""## <font color="#004D7F" size=4>Ejercicio 5</font>
Compara los resultados obtenidos en cada una de las arquitecturas definidas y quédate con la mejor. **¿En qué experimento se obtiene los mejores resultados?**

La arquitectura elegida la usaremos en el caso práctico para seguir ajustando nuestro modelo y alcanzar un buen rendimiento.


### **Análisis Comparativo**

Se analizan los tres modelos segun sus metricas:

 **`loss`** (mean_absolute_percentage_error), **`mae`**, **`val_loss`**, y **`val_mae`**.

---

#### **Modelo 1**

- **`loss` (entrenamiento):** 96.3094  
- **`val_loss` (validación):** 95.8981  
- **`mae` (entrenamiento):** 202,316.22  
- **`val_mae` (validación):** 200,796.44  

- El **MAE** es muy alto (200,000), lo que sugiere predicciones con errores, el alto error absoluto lo hace inservible como el mejor modelo.

---

#### **Modelo 2**

- **`loss` (entrenamiento):** 33.5313  
- **`val_loss` (validación):** 34.1637  
- **`mae` (entrenamiento):** 58,774.62  
- **`val_mae` (validación):** 59,747.57  


Generalización sólida, por las metricas, el **MAE** es menor (59,000) en comparación con el Modelo 1.  

Aunque es mejor que el Modelo 1, el **MAE** sigue siendo relativamente alto para aplicaciones que requieren alta precisión.  Es un Buen modelo que tiene un balance aceptable entre pérdida y error absoluto, pero no seria el mejor.

---

#### **Modelo 3**

- **`loss` (entrenamiento):** 25.3974  
- **`val_loss` (validación):** 25.8387
- **`mae` (entrenamiento):** 49,985.11  
- **`val_mae` (validación):** 55,148.70  

Menor **MAE** promedio (50,000-55,000) tiene una mejor generalización pues estan cerca las de entrenamiento, sin indicios de sobreajuste.

Aunque el error es más bajo, aunque es grande aun.  

**Conclusión:**  

Es el mejor modelo, ya que logra el menor **MAE** y una pérdida más baja,ademas que logra una buena generalización pero se necesita ajustar los datos o la arquitectura.
"""

import matplotlib.pyplot as plt
# Graficar la pérdida de los modelos
plt.plot(Prueba_1.history['loss'], label='modelo 1')  # Correcto, accedemos al .history
plt.plot(Prueba_2.history['loss'], label='modelo 2')
plt.plot(Prueba_3.history['loss'], label='modelo 3')  # Accede al objeto history correctamente
plt.plot(Prueba_1.history['val_loss'], label='modelo 1')  # Correcto, accedemos al .history
plt.plot(Prueba_2.history['val_loss'], label='modelo 2')
plt.plot(Prueba_3.history['val_loss'], label='modelo 3')  # Accede al objeto history correctamente
plt.title('Progreso de la Pérdida durante el Entrenamiento')
plt.xlabel('Épocas')
plt.ylabel('Pérdida')
plt.legend()
plt.show()

"""# Experimento 1: Probar diferentes tamaños de batch"""

# Definición del modelo
model_4 = Sequential()

# Añadimos las capas
model_4.add(Dense(9, activation='relu', input_dim=x_Quitotrain.shape[1]))  # Capa de entrada
model_4.add(Dense(128, activation='relu'))  # Segunda capa
model_4.add(Dropout(0.01))
model_4.add(Dense(64, activation='relu'))   # Tercera capa
model_4.add(Dropout(0.0001))
model_4.add(Dense(32, activation='relu'))   # Cuarta capa
model_4.add(Dropout(0.001))
model_4.add(Dense(16, activation='relu'))   # Quinta capa
model_4.add(Dense(1))  # Capa de salida (sin activación para problemas de regresión)

# Configuración del optimizador
optimizador_4 = Adam(learning_rate=0.0001)
model_4.compile(optimizer=optimizador_4, loss='mean_absolute_percentage_error', metrics=['mae'])

# Experimento 1: Batch size = 58, epochs = 45
print("Entrenamiento con batch_size=58")
experimento_1 = model_4.fit(
    x_Quitotrain, y_Quitotrain,
    epochs=45,
    batch_size=58,
    verbose=1
)

# Evaluar el modelo con batch_size=36
print("\nEvaluación con batch_size=36")
loss_4_si, mae_4_si = model_4.evaluate(x_Quitotrain, y_Quitotrain, batch_size=36)

# Experimento 2: Batch size = 64, epochs = 36
print("\nEntrenamiento con batch_size=64")
Experimento_2 = model_4.fit(
    x_Quitotrain, y_Quitotrain,
    epochs=36,
    batch_size=64,
    verbose=1
)

# Evaluar el modelo con batch_size=64
print("\nEvaluación con batch_size=64")
loss_4_size64, mae_4_size64 = model_4.evaluate(x_Quitotrain, y_Quitotrain, batch_size=64)

# Resultados finales
print(f"\nResultados finales:")
print(f"Batch size 36, Epoch 58  y learning_rate=0.0001 - Pérdida (MAPE): {loss_4_si:.4f}, MAE: {mae_4_si:.2f}")
print(f"Batch size 64, Epoch 45  y learning_rate=0.0001 - Pérdida (MAPE): {loss_4_size64:.4f}, MAE: {mae_4_size64:.2f}")

import matplotlib.pyplot as plt

# Visualización de las métricas
def plot_metrics(history, batch_size_label):
    fig, ax1 = plt.subplots(figsize=(6, 4))

    # Eje izquierdo para MAE
    ax1.plot(history['mae'], label=f'Train MAE ({batch_size_label})', linestyle='--', color='blue')
    ax1.set_xlabel('Épocas')
    ax1.set_ylabel('MAE', color='blue')
    ax1.tick_params(axis='y', labelcolor='blue')

    # Eje derecho para MAPE
    ax2 = ax1.twinx()
    ax2.plot(history['loss'], label=f'Train MAPE ({batch_size_label})', linestyle='-', color='orange')
    ax2.set_ylabel('MAPE', color='orange')
    ax2.tick_params(axis='y', labelcolor='orange')

    # Título y ajuste del diseño
    plt.title(f'Evolución de Métricas para {batch_size_label}')
    fig.tight_layout()
    plt.show()

# Llamar a la función para cada experimento
plot_metrics(experimento_1.history, 'Batch size 36, Epoch 58  y learning_rate=0.0001 - Pérdida (MAPE)')
plot_metrics(Experimento_2.history, 'Batch size 64, Epoch 45  y learning_rate=0.0001 - Pérdida (MAPE)')

"""# **Comparación de configuraciones:**
#### ** Batch size = 36, Epoch = 58, Learning rate = 0.0001**
- **`loss (MAPE):`** 97.4049
- **`mae:`** 202,012.03

El modelo necesitó **58 époch** para alcanzar su mejor rendimiento, tanto el MAPE como el MAE son altos lo que nos muestra que es menos eficiente, comparada con el modelo de batch size 64 que veremos mas adelante..


#### ** Batch size = 64, Epoch = 45, Learning rate = 0.0001**
- **`loss (MAPE):`** 63.9366
- **`mae:`** 152,556.52

Si se reducen las epocas a **45** y se utiliza un batch size más grande se ve el cambio y la mejora tanto el MAPE como el MAE. Esto nos dice que esta configuracion logra rescatar menores errores absolutos y relativos en las predicciones.



### **Conclusión:**

El modelo entrenado con **batch size = 64** y menos **45 épochc** aparentemente es mas eficiente no ovidemos que se agrego tambien Droup para reforzar las capas

- **MAPE** más bajo (**63.9**) indica menor error relativo.
- **MAE** más bajo (**152,556.52**) significa que las predicciones son más precisas en términos absolutos.

- Un batch size más grande nos dice que puede mejorar el modelo.

Esto nos dice que para mejorar un modelo podriamos empezar por el **batch size mayor** y reducir el **menos époch** ademas intentarlos con grandes y pequeñas muestras.

#**Machine Learning Creativo**

En este segmento crearemos unas pruebas para validar las teorias que se conocieron anteriormente. Ademas entrenaremos muestras de la base de datos para hacer que sean interesantes las redes neuronales redes neuronales.
"""

datos3

x = datos3[['longitude', 'latitude', 'housing_median_age', 'total_rooms',
       'total_bedrooms', 'population', 'households', 'median_income',
       'ocean_proximity']].values
y = datos3[['median_house_value']].values

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
# Codificar la columna categórica 'ocean_proximity' con OneHotEncoder
encoder = OneHotEncoder(sparse_output=False, drop='first')
x_categorical = encoder.fit_transform(x[:, [-1]])  # La última columna es 'ocean_proximity'

# Concatenar las columnas codificadas con las demás características
x = np.hstack((x[:, :-1], x_categorical))  # Excluye la última columna y añade las codificadas

# Dividir los datos en 80% para entrenamiento y 30% para prueba
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123)

print(x_train.shape) # Tamaño x de entrenamiento
print(y_train.shape) # Tamaño y de entrenamiento
print(x_test.shape) # Tamaño x de test
print(y_test.shape) # Tamaño y de test

actual_loss = 'mean_absolute_percentage_error'

# Escalar las características
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Definición del modelo
model_5 = Sequential()

# Añadimos las capas
model_5.add(Dense(480, activation='relu', input_dim=x_train.shape[1]))  # Capa de entrada
model_5.add(Dropout(0.0001))
model_5.add(Dense(360, activation='relu'))  # Segunda capa
model_5.add(Dropout(0.0001))
model_5.add(Dense(240, activation='relu'))  # Tercera capa
model_5.add(Dropout(0.0001))
model_5.add(Dense(128, activation='relu'))  # Cuarta capa
model_5.add(Dropout(0.0001))
model_5.add(Dense(64, activation='relu'))   # Quinta capa
model_5.add(Dropout(0.0001))
model_5.add(Dense(32, activation='relu'))   # Sexta capa
model_5.add(Dropout(0.0001))
model_5.add(Dense(16, activation='relu'))   # Septima capa
model_5.add(Dense(1))  # Capa de salida (sin activación para problemas de regresión)

# Configuración del optimizador
optimizador_5 = Adam(learning_rate=0.0001)
model_5.compile(optimizer=optimizador_5, loss=actual_loss, metrics=['mae'])

# Experimento 1: Batch size = 150, epochs = 32
print("Entrenamiento con batch_size=58")
experimento_2 = model_5.fit(
    x_train, y_train,
    epochs=32,
    batch_size=150,
    verbose=1,validation_data=(x_test, y_test )
)

# Experimento 2: Batch size = 250, epochs = 32
print("Entrenamiento con batch_size=58")
experimento_3 = model_5.fit(
    x_train, y_train,
    epochs=32,
    batch_size=250,
    verbose=1, validation_data=(x_test, y_test )
)

# Experimento 3: Batch size = 450, epochs = 32
print("Entrenamiento con batch_size=58")
experimento_4 = model_5.fit(
    x_train, y_train,
    epochs=32,
    batch_size=450,
    verbose=1, validation_data=(x_test, y_test )
)

# Experimento 4: Batch size = 550, epochs = 32
print("Entrenamiento con batch_size=58")
experimento_5 = model_5.fit(
    x_train, y_train,
    epochs=32,
    batch_size=450,
    verbose=1, validation_data=(x_test, y_test)
)

"""### **Análisis de Resultados de los Experimentos de Batch Size**
Probamos 4 experimentos con distintos bach (`batch_size`) y un modelo con Droup  en el rendimiento del modelo, manteniendo constantes el número de époch (`epochs = 32`) dado a que si se aumenta se vuelve ineficiente. En la siguiente tabla se muestran los datos de: **`loss`** (MAPE) y **`mae`** (error absoluto medio).

---

### **Análisis de los Resultados de los Experimentos con Conjuntos de Validación**

En estos experimentos, se evaluó el impacto de diferentes tamaños de batch sobre las métricas de pérdida (**`loss`**, medida como el error porcentual medio absoluto o **MAPE**) y el error absoluto medio (**MAE**) tanto en entrenamiento como en validación. A continuación, se realiza un análisis detallado.

---

### **1. Resumen de Resultados**
| Experimento | Batch Size | Epochs | `Loss` (MAPE) | `MAE` (Entrenamiento) | `Val_Loss` (MAPE) | `Val_MAE` (Validación) |
|-------------|------------|--------|---------------|-----------------------|-------------------|-------------------------|
| Experimento 1 | 150        | 32     | 17.7048       | 38,388.06            | 18.6082           | 38,738.46              |
| Experimento 2 | 250        | 32     | 17.1500       | 36,511.04            | 18.4603           | 37,692.60              |
| Experimento 3 | 450        | 32     | 16.8182       | 35,933.20            | 18.3401           | 37,391.99              |
| Experimento 4 | 550        | 32     | 16.662       | 35,242.09            | 18.078           | 37137.65              |


---

### **Rendimiento del Modelo:**

   - A medida que se incrementa el tamaño de batch, tanto el `loss` (MAPE) como el `MAE` disminuyen de manera consistente.

   - Esto nos dice que entre mas batches grandes, resulta en una mejor convergencia.

2. **Duración del Entrenamiento:**

Aumentar el tamaño del batch reduce el número de pasos por époch.

   - Por ejemplo:
     - Con **batch size = 150**, el modelo realiza **96 pasos** por époch.
     - Con **batch size = 550**, el modelo realiza solo **32 pasos** por époch. Aunque con un batch mas grande se aumenta la necesidad de computo.


### **3. Conclusión**

**Mejor modelo: Es ** **Batch size = 550** con **32 époch** proporciona esto se debe a que proporciona un mejor equilibrio y logra estabilizar el proceso de entrenamiento las métricas de entrenamiento (`loss: 16.662`, `mae: 35,242.09`) y validación (`val_loss: 18.078`, `val_mae: 37,137.65`).

El modelo muestra un buen desempeño en tareas de regresión, pero el ajuste fino de los hiperparámetros podría seguir mejorando su precisión.
"""

# Graficar el progreso de la pérdida durante el entrenamiento
plt.figure(figsize=(8, 6))
plt.plot(experimento_2.history['loss'], label='modelo 1 ')
plt.plot(experimento_3.history['loss'], label='modelo 2 ')
plt.plot(experimento_4.history['loss'], label='modelo 3 ')
plt.plot(experimento_5.history['loss'], label='modelo 4 ')
plt.title('Progreso de la Pérdida durante el Entrenamiento')
plt.xlabel('Épocas')
plt.ylabel('Pérdida')
plt.legend()
plt.grid(True)
plt.show()

# Graficar el progreso de la pérdida durante el entrenamiento
plt.figure(figsize=(8, 6))
plt.plot(experimento_2.history['val_loss'], label='modelo 1 ')
plt.plot(experimento_3.history['val_loss'], label='modelo 2 ')
plt.plot(experimento_4.history['val_loss'], label='modelo 3 ')
plt.plot(experimento_5.history['val_loss'], label='modelo 4 ')
plt.title('Progreso de la Pérdida durante el Entrenamiento')
plt.xlabel('Épocas')
plt.ylabel('Pérdida')
plt.legend()
plt.grid(True)
plt.show()